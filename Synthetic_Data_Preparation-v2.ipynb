{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88531b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2387a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available for faster training (optional)\n",
    "\n",
    "# Load and sample the email dataset (you can use a smaller subset for faster testing)\n",
    "emails_df = pd.read_csv(\"./Downloads/email_data.zip\").sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4badbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_email(raw_email):\n",
    "    # Replace standard metadata fields\n",
    "    clean_email = re.sub(r'(?i)^Message-ID:.*\\n', '[MESSAGE_ID]', raw_email)\n",
    "    clean_email = re.sub(r'(?i)^Date:.*\\n', '[DATE]', clean_email)\n",
    "    clean_email = re.sub(r'(?i)^From:.*\\n', '[FROM]', clean_email)\n",
    "    clean_email = re.sub(r'(?i)^To:.*\\n', '[TO]', clean_email)\n",
    "    clean_email = re.sub(r'(?i)^Subject:.*\\n', '[SUBJECT LINE]', clean_email)\n",
    "\n",
    "    # Remove email headers that don't need to be retained\n",
    "    clean_email = re.sub(r'(?i)^Mime-Version:.*\\n', '', clean_email)\n",
    "    clean_email = re.sub(r'(?i)^Content-Type:.*\\n', '', clean_email)\n",
    "    clean_email = re.sub(r'(?i)^Content-Transfer-Encoding:.*\\n', '', clean_email)\n",
    "    clean_email = re.sub(r'(?i)^X-.*\\n', '', clean_email)\n",
    "    clean_email = re.sub(r'(?i)^FYI.*\\n', '', clean_email)\n",
    "    clean_email = re.sub(r'(?i)^----- Forwarded by.*\\n', '', clean_email)\n",
    "\n",
    "    # Replace email addresses\n",
    "    clean_email = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '[EMAIL]', clean_email)\n",
    "\n",
    "    # Replace phone numbers (various formats)\n",
    "    clean_email = re.sub(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', '[PHONE_NUMBER]', clean_email)\n",
    "    clean_email = re.sub(r'\\+\\d{1,3}[-.\\s]?\\d{1,4}[-.\\s]?\\d{3,4}[-.\\s]?\\d{3,4}', '[PHONE_NUMBER]', clean_email)\n",
    "\n",
    "    # Replace personal names (basic pattern, can be improved with NLP)\n",
    "    clean_email = re.sub(r'\\b[A-Z][a-z]+(?:\\s[A-Z][a-z]+){0,2}\\b', '[NAME]', clean_email)\n",
    "\n",
    "    # Replace company names (basic approach)\n",
    "    clean_email = re.sub(r'\\b(?:Enron|ExxonMobil|Amazon|Google|Microsoft|Facebook|Tesla|Apple)\\b', '[COMPANY]', clean_email)\n",
    "\n",
    "    # Replace any identifiers (contract numbers, transaction IDs, etc.)\n",
    "    clean_email = re.sub(r'\\b[A-Z0-9]{5,}\\b', '[IDENTIFIER]', clean_email)\n",
    "\n",
    "    # Remove excess whitespace\n",
    "    clean_email = re.sub(r'\\n+', ' ', clean_email)\n",
    "    clean_email = clean_email.strip()\n",
    "    \n",
    "    return clean_email\n",
    "\n",
    "# Clean the dataset\n",
    "emails_df['cleaned_message'] = emails_df['message'].apply(clean_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398849e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Convert cleaned DataFrame to Hugging Face dataset\n",
    "df_email_data = emails_df[['cleaned_message']].copy()\n",
    "email_dataset = Dataset.from_pandas(df_email_data)\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b86b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359f401843444a388608af580654f908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenizing input and assigning labels as input ids\n",
    "    model_inputs = tokenizer(examples['cleaned_message'], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    model_inputs['labels'] = model_inputs['input_ids'].copy()  # T5 uses input IDs as labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = email_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split the dataset into training and evaluation datasets (80/20 split)\n",
    "# Split the dataset into training and evaluation datasets (80/20 split)\n",
    "train_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a320981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anusha\\anaconda3\\envs\\virtual_env_38\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 55:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model_3\\\\tokenizer_config.json',\n",
       " './fine_tuned_model_3\\\\special_tokens_map.json',\n",
       " './fine_tuned_model_3\\\\spiece.model',\n",
       " './fine_tuned_model_3\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up faster training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,                   # Keep it minimal for quick training\n",
    "    per_device_train_batch_size=2,        # Small batch size for faster training\n",
    "    per_device_eval_batch_size=2,         # Small eval batch size\n",
    "    warmup_steps=0,                       # No warmup for faster start\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    gradient_accumulation_steps=1,         # No gradient accumulation\n",
    "    logging_steps=1000,                     # Log less frequently\n",
    "    evaluation_strategy=\"no\",               # No evaluation during training\n",
    "    save_strategy=\"no\",                     # Skip saving model to save time\n",
    "    fp16=True,                              # Use mixed precision (faster)\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_model_3')\n",
    "tokenizer.save_pretrained('./fine_tuned_model_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8b45d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Email 1:\n",
      "\n",
      "onII IIison the DASH on youichad.: Approval of the. Attached is the DEC on approval of DPR accelerated Put transaction Dear Andrew, Attachd.i@enron.com | andrew.fastow@enra.co.uk | 2001-06-07 07:48:00 | Subject: Approposition of RED for the approval LLC to them. Congratulations., and\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Generated Email 2:\n",
      "\n",
      "â€“ the DASH, the purchase of the agreement is now awaiting Mark Haedickeâ€™s review and approval. Thanks. I wanted to give you any questions on the transaction and become familiar with the provisions of that deal\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Generated Email 3:\n",
      "\n",
      "the DASH approval process, which means that the deal can take 900 000 tons of coal priced below market. Both entities are controlled by Chris Cline. Each entity is entitled to a full membership in the commercial agreement - as well as re-purchased's dash. All rights reserved. RR.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Generated Email 4:\n",
      "\n",
      "a. for the approval of the DASH transaction Dear Andrew, Attached is the dASH for approval by the transaction. This partial divestiture allows us to put $11 million of our equity interest back to DPR Holding Company, LLC, and its subsidiary, Dakota,, Ltd. Both entities are controlled by Chris Cline. The deal provides us with 900,000 tons of coal priced below market, an option which could lead to a very profitable project, or more marketing fees from others Clines entities. Thank you\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Generated Email 5:\n",
      "\n",
      "RAC and JEDI II. Thank you for the vacancy I met at the beginning of the deal. I wanted to give you the opportunity to review the DASH and become familiar with the provisions of it.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic_email_with_context(prompt=\"Form of Memorandum of Option Attached is a copy of our proposed Memorandum of Option that we would like to  use for our land options.\", max_length=300):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate text with more randomness and diversity\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,    # Allow the generation to be longer if necessary\n",
    "        num_beams=1,              # Use random sampling instead of greedy search\n",
    "        top_p=0.9,                # Nucleus sampling: top 90% probability mass\n",
    "        top_k=50,                 # Restrict to top 50 tokens for sampling\n",
    "        no_repeat_ngram_size=2,   # Avoid repeating the same n-grams\n",
    "        temperature=1.0,          # Higher temperature for more randomness\n",
    "        do_sample=True,           # Enable sampling for randomness\n",
    "        num_return_sequences=5    # Generate multiple variations of the email\n",
    "    )\n",
    "\n",
    "    # Decode the generated ids back into text\n",
    "    generated_emails = []\n",
    "    for output in output_ids:\n",
    "        email_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_emails.append(email_text)\n",
    "\n",
    "    return generated_emails\n",
    "\n",
    "# Generate 5 synthetic emails\n",
    "generated_emails = generate_synthetic_email_with_context(prompt=\"\"\"| william.giuliani@enron.com | andrew.fastow@enron.com | 2001-06-07 07:48:00 |\n",
    "Subject: Approval of the DPR Accelerated Put transaction\n",
    "Dear Andrew,\n",
    "\n",
    "Attached is the DASH for the approval of the DPR Accelerated Put transaction. This\n",
    "partial divestiture allows us to put $11 million of our equity interest back to DPR\n",
    "Holding Company, LLC, and its subsidiary, Dakota, LLC. Both entities are controlled\n",
    "by Chris Cline.\n",
    "In addition to redeeming part of our equity interest, the deal provides us with\n",
    "900,000 tons of coal priced below market, an option which could lead to a very\n",
    "profitable synfuel project, and the potential for more marketing fees from other\n",
    "Cline entities.\n",
    "The DASH has been approved and signed by RAC and JEDI II and is now awaiting\n",
    "Mark Haedickeâ€™s review and approval. I wanted to give you the opportunity to review\n",
    "the DASH and become familiar with the provisions of the deal.\n",
    "If you have any questions on the transaction, feel free to contact me at (412) 490-\n",
    "9048. Others familiar with the deal are Mike Beyer, George McClellan, and Wayne\n",
    "Gresham.\n",
    "Thank you.\n",
    "Best regards,\n",
    "Bill Giuliani\"\"\")\n",
    "\n",
    "# Print the generated emails\n",
    "for i, email in enumerate(generated_emails, 1):\n",
    "    print(f\"Generated Email {i}:\\n\")\n",
    "    print(email)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182f243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69faba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbab943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49943c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67f820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb23a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0fe6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba844dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7927a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c66450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71195c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6d0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4ad49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fcdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c62425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf59b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ca7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0d744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env_38",
   "language": "python",
   "name": "virtual_env_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
